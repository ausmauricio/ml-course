{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trabalho Prático 2 - Aprendizado de Máquinas\n",
    "\n",
    "#### Aluno: Mauricio de Oliveira\n",
    "#### Professor: Adriano Veloso\n",
    "#### Departamento de Ciência da Computação - Universidade Federal de Minas Gerais\n",
    "\n",
    "### RESUMO\n",
    "\n",
    "#### Este trabalho prático se refere a implementação da técnica de _boosting_, processo que adiciona diversos classificadores fracos de modo iterativo para construir um classificador forte. O processo de _boosting_ emprega dois mecanismos: ajustes nos pesos de cada classificador e ajustes nos pesos associados a cada entrada. \n",
    "#### Os conjuntos de pesos são alterados a cada iteração $t$ do processo. O peso de cada classificador indica a importância deste para classificação binária correta $(+1, -1)$. Assim, o peso $\\alpha^t$ de um classificador será alto caso seu erro (_training error_ ou erro empírico) for baixo. Já o ajuste no peso $w_i$ de cada entrada do _training set_ é feito de modo que exemplos classificados incorretamente tenha seu peso aumentado e exemplos com classificação correta tenha o peso diminuído, pois dessa forma, na próxima iteração, será possível dar maior foco aos exemplos incorretos.\n",
    "#### Sendo assim, o processo iterativo do _boosting_ é o seguinte:\n",
    "\n",
    "> 1. No início, fazer $w_i^1$ = 1/n, sendo n o tamanho do _training set_ <br>\n",
    "> 2. Loop: <br>\n",
    ">> 2.1 Escolha classificador $h_t$ que minimiza erro empírico na iteração t <br>\n",
    ">> 2.2 Recalcule $\\alpha^t$ <br>\n",
    ">> 2.3 Recalcule $w_i^{t+1} $ <br>\n",
    "> 3. Parar quando erro empírico se aproxima de zero. <br>\n",
    "> 4. O classificador final treinado pelo processo de boosting será dado por:\n",
    "\n",
    "\\begin{equation*}\n",
    "h(X) = \\sum_{t=1} sign(\\alpha^t h_t(X))\n",
    "\\end{equation*}\n",
    "\n",
    "### DETALHES\n",
    "\n",
    "#### Da sequência acima, é preciso definir $h_t$, como é feito o passo 2.2 e 2.3.\n",
    "#### 1. $h_t$ pode ser qualquer classificador fraco, e neste trabalho será usado [_decision stumps_](https://en.wikipedia.org/wiki/Decision_stump), uma _decision tree_ de profundidade 1. <br> A escolha se baseia no erro empírico de cada _decision stump_ analisado, levando em consideração o peso $w_i$. Assim, um classificador que faz dois erros em entradas cujo pesos são 0.5 e 1, respectivamente, tem erro menor do que um classificador que erra em apenas 1 exemplo, porém com peso 2. \n",
    " \n",
    "#### 2. O ajuste de $\\alpha^t$ (passo 2.2) é feito da seguinte forma:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\alpha^t = \\dfrac{1}{2} \\log_{} {\\dfrac{1-\\varepsilon^t}{\\varepsilon^t}}\n",
    "\\end{equation*}\n",
    "\n",
    "#### na qual $\\varepsilon^t$ é o erro empírico do modelo na iteração t (que já pode ser calculado logo após passo 2.1)\n",
    "#### 3. Por último, o ajuste nos pesos dos exemplos para a próxima iteração $w_i^{t+1} $ é dado por:\n",
    "\n",
    "\\begin{equation*}\n",
    "w_i^{t+1} = w_i^t e^{-\\alpha^t h_t y}\n",
    "\\end{equation*}\n",
    "\n",
    "#### aonde y é o valor correto {+1,-1}. É importante normalizar (dividir pela soma) $w_i^{t+1}$ de modo que $ \\sum_{i=1}^n w_i^{t+1} = 1$. Note que dessa forma o passo 2.3 de fato irá elevar o peso dos exemplos incorretos e diminuir dos exemplos corretos. Por exemplo, se $h_t$ está errado, isso significa que $sign(h_t y) = -1$. Assim, $w_i^{t+1}$ será aumentado por um fator de $e^{\\alpha^t}$. Caso contrário, $w_i^{t+1}$ será diminuido ao ser multiplicado por $e^{-\\alpha^t}$. <br> Obs: essa análise supõe $\\alpha^t$ > 0, o que ocorrerá caso $\\varepsilon^t < 0.5$, ou seja, nós conseguimos encontrar classificadores fracos mas que ao menos possuem desempenho um pouco melhor que um classificador aleatório.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
